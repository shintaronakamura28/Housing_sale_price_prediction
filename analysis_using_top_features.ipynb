{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as mn\n",
    "\n",
    "#preprocessing\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, learning_curve\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import ColumnTransformer, make_column_selector\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.exceptions import NotFittedError\n",
    "import sklearn\n",
    "#models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from lazypredict import Supervised\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import ExtraTreesRegressor, AdaBoostRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import Lasso, Ridge\n",
    "\n",
    "\n",
    "# machine learning library\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.metrics import(recall_score, accuracy_score, f1_score, precision_score, confusion_matrix, classification_report, ConfusionMatrixDisplay, roc_curve, auc)\n",
    "\n",
    "#SHAP explainer\n",
    "import shap\n",
    "# Ensure your pipeline is defined\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#LIME\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "#widgets and dispaly\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "#utilities\n",
    "import pickle\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'data/top_10_features_dataset.csv'\n",
    "\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_record = df.shape[0]\n",
    "num_features = df.shape[1]\n",
    "data_types = df.dtypes\n",
    "\n",
    "print(f'Number of records: {num_record}')\n",
    "print(f'\\nNumber of features {num_features}')\n",
    "print(f'\\nData types: \\n{data_types}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize missing values\n",
    "mn.matrix(df, figsize=(10,5), width_ratios=(5,1), fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicated_rows = df.duplicated().sum()\n",
    "missing_values = df.isna().sum()\n",
    "\n",
    "print(f'Number of duplicated rows: {duplicated_rows}')\n",
    "print(f'\\nNumber of missing values: \\n{missing_values}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "statistics = df.describe()\n",
    "print(f'\\nStatistics: {statistics}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_column_values(df):\n",
    "    for column in df.columns:\n",
    "        print(f'Unique Values in {column} column:')\n",
    "        print(df[column].unique())\n",
    "        print('\\n')\n",
    "\n",
    "check_column_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cols = df.select_dtypes(include='number')\n",
    "\n",
    "corr = num_cols.corr()\n",
    "\n",
    "# Creating the heatmap\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap='coolwarm', cbar=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_corr = corr['SalePrice'].sort_values(ascending=False)\n",
    "\n",
    "# Display top 10 features\n",
    "top_10_features = target_corr[1:11]  # Exclude 'SalePrice' itself\n",
    "print(top_10_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Size of Correlation | Interpretation                        |\n",
    "|---------------------|----------------------------------------|\n",
    "| .90 to 1.00 (-.90 to -1.00) | Very high positive (negative) correlation |\n",
    "| .70 to .90 (-.70 to -.90)   | High positive (negative) correlation      |\n",
    "| .50 to .70 (-.50 to -.70)   | Moderate positive (negative) correlation  |\n",
    "| .30 to .50 (-.30 to -.50)   | Low positive (negative) correlation       |\n",
    "| .00 to .30 (.00 to -.30)    | Negligible correlation                    |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "1. **SqFtTotLiving (0.51):**\n",
    "   - **Moderate positive correlation:** The total living space has a moderately strong relationship with sale price. As living space increases, sale price tends to increase.\n",
    "\n",
    "2. **Latitude (0.41):**\n",
    "   - **Low positive correlation:** Latitude has a weak positive relationship with sale price. As latitude increases, sale price tends to increase slightly.\n",
    "\n",
    "3. **SqFt2ndFloor (0.37):**\n",
    "   - **Low positive correlation:** The second floor's square footage has a weak positive relationship with sale price. As the second floor's size increases, sale price tends to increase slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = [col.strip() for col in df.columns]\n",
    "\n",
    "# check updated column names\n",
    "print(df.columns)\n",
    "\n",
    "# check updated column names\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# check for duplicates\n",
    "print(f'\\nnumber of duplicate rows: {df.duplicated().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Overview\n",
    "df.hist(figsize=(15,12), bins=10, grid=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'SalePrice'\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, figsize=(6,6), sharex=True)\n",
    "sns.histplot(df[feature], bins='auto', kde=True, ax=axes[0])\n",
    "axes[0].tick_params(axis='x', rotation=45)\n",
    "axes[0].set_title(f'Histogram of {feature}')\n",
    "\n",
    "sns.boxplot(data=df, x=feature, ax=axes[1])\n",
    "#remove grids\n",
    "axes[0].grid(False)\n",
    "axes[1].grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "- The distribution of sale prices is right-skewed, meaning that most of the sale prices are concentrated on the lower end, with a long tail extending to the right.\n",
    "- There is a peak around $500,000, indicating that this price range has the highest frequency of sales.\n",
    "- The highest frequency of sale prices falls between $400,000 and $600,000, indicating that most houses are sold within this range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the original distribution of SalePrice\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(df['SalePrice'], bins=50, color='blue', alpha=0.7)\n",
    "plt.title('Original SalePrice Distribution')\n",
    "plt.xlabel('SalePrice')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scatter Plots and Regression Lines for Features vs Sale Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_features = ['SqFtTotLiving', 'SqFt2ndFloor','SqFtOpenPorch', 'SqFtFinBasement']\n",
    "for feature in numerical_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(x=df[feature], y=df['SalePrice'])\n",
    "    sns.regplot(x=df[feature], y=df['SalePrice'], scatter=False, color='red')\n",
    "    plt.title(f'Sale Price vs {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('SalePrice')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "#### Sale Price vs SqFtTotLiving üè†\n",
    "- **Interpretation**: The scatter plot shows a strong positive correlation between the total living area (SqFtTotLiving) and the sale price. As the living area increases, the sale price also tends to increase. The red regression line reinforces this positive trend, indicating that larger living spaces are generally associated with higher sale prices.\n",
    "\n",
    "#### Sale Price vs SqFt2ndFloor üè¢\n",
    "- **Interpretation**: The scatter plot suggests a moderate positive correlation between the second floor area (SqFt2ndFloor) and the sale price. Homes with more second-floor space tend to have higher sale prices. However, there is a significant number of homes with zero second-floor space, showing that many homes do not have a second floor. The regression line shows an upward trend.\n",
    "\n",
    "#### Sale Price vs SqFtOpenPorch üåû\n",
    "- **Interpretation**: There is a weak positive correlation between the open porch area (SqFtOpenPorch) and the sale price. Homes with larger open porch areas tend to have slightly higher sale prices, but the relationship is not as strong as with living area or second-floor space. The regression line indicates a positive but less pronounced trend.\n",
    "\n",
    "#### Sale Price vs SqFtFinBasement üè°\n",
    "- **Interpretation**: The scatter plot shows a weak to moderate positive correlation between the finished basement area (SqFtFinBasement) and the sale price. Homes with more finished basement space tend to have higher sale prices, but the relationship is not very strong. The regression line shows an upward trend, suggesting that finished basements do add value to homes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_threshold = df['SalePrice'].quantile(0.99)\n",
    "max_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['SalePrice']>max_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_threshold = df['SalePrice'].quantile(0.01)\n",
    "min_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['SalePrice']<min_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_threshold, max_threshold = df['SalePrice'].quantile([0.00001, 0.957])\n",
    "min_threshold, max_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df[(df['SalePrice'] < max_threshold) & (df['SalePrice'] > min_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix, axes = plt.subplots(nrows=2, figsize=(6,8), sharex=False)\n",
    "sns.set_style('white')\n",
    "\n",
    "#histogram\n",
    "sns.histplot(df2['SalePrice'], bins='auto', kde=True, ax=axes[0])\n",
    "axes[0].tick_params(axis='x', rotation=90, labelrotation=45)\n",
    "\n",
    "#box plot\n",
    "sns.boxplot(data=df2, x='SalePrice', ax=axes[1])\n",
    "plt.tight_layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Addressing Skewness in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['LogSalePrice'] = np.log(df2['SalePrice'])\n",
    "\n",
    "sns.histplot(df2['LogSalePrice'], kde=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Machine Learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split datab\n",
    "y = df2['LogSalePrice']\n",
    "X = df2.drop(['LogSalePrice'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "#use Isolation forest to remove outliers\n",
    "iso = IsolationForest(contamination=0.1)\n",
    "yhat = iso.fit_predict(X_train)\n",
    "mask = yhat != -1\n",
    "\n",
    "#apply the mask to filter the dataframe rows\n",
    "\n",
    "X_train_clean = X_train[mask]\n",
    "y_train_clearn = y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled  = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Supervised.removed_regressors.append(\"QuantileRegressor\")\n",
    "Supervised.REGRESSORS.remove(('QuantileRegressor', sklearn.linear_model._quantile.QuantileRegressor))\n",
    "lazy_reg = Supervised.LazyRegressor(verbose=0, ignore_warnings=True, custom_metric=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Fit LazyRegressor on the scaled data\n",
    "models, predictions = lazy_reg.fit(X_train_scaled, X_test_scaled, y_train, y_test)\n",
    "\n",
    "# Display the results\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_df = models\n",
    "\n",
    "top_20_models = models_df.sort_values(by='R-Squared', ascending=False).head(20)\n",
    "top_20_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mse, rmse, mae, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet, BayesianRidge\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'ElasticNet': ElasticNet(),\n",
    "    'BayesianRidge': BayesianRidge(),\n",
    "    'SVR': SVR()\n",
    "}\n",
    "\n",
    "param_grid = {\n",
    "    'ElasticNet':{\n",
    "        'regressor__alpha': [0.1, 1.0, 10.0, 100.0],  # Regularization strength\n",
    "        'regressor__l1_ratio': [0.1, 0.5, 0.7, 0.9, 1.0]  # Balance between L1 and L2 regularization\n",
    "},\n",
    "    'SVR': {\n",
    "        'regressor__C': [0.1, 1, 10],\n",
    "        'regressor__epsilon': [0.01, 0.1, 0.2],\n",
    "        'regressor__kernel': ['linear', 'rbf']\n",
    "    },\n",
    "    'BayesianRidge': {\n",
    "        'regressor__alpha_1': [1e-6, 1e-5, 1e-4],\n",
    "        'regressor__alpha_2': [1e-6, 1e-5, 1e-4],\n",
    "        'regressor__lambda_1': [1e-6, 1e-5, 1e-4],\n",
    "        'regressor__lambda_2': [1e-6, 1e-5, 1e-4]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a loop function to train and evaluate models\n",
    "metrics_list = []\n",
    "best_models = {}\n",
    "\n",
    "def train_and_evaluate_models(X_train, y_train, X_test, y_test, preprocessor):\n",
    "    for model_name, model in tqdm(models.items(), desc='Training Models'):\n",
    "        model_pipe = Pipeline(steps=[('regressor', model)])\n",
    "        \n",
    "        # Perform GridSearchCV\n",
    "        grid_search = GridSearchCV(\n",
    "            estimator=model_pipe,\n",
    "            param_grid=param_grid[model_name],\n",
    "            cv=5,\n",
    "            n_jobs=-1,\n",
    "            scoring='r2')\n",
    "        \n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        best_model = grid_search.best_estimator_\n",
    "        best_models[model_name] = best_model\n",
    "        \n",
    "        # Predictions\n",
    "        train_pred = best_model.predict(X_train)\n",
    "        test_pred = best_model.predict(X_test)\n",
    "        \n",
    "        # Evaluate classification models\n",
    "        train_mse, train_rmse, train_mae, train_r2 = evaluate_regression(y_train, train_pred)\n",
    "        test_mse, test_rmse, test_mae, test_r2 = evaluate_regression(y_test, test_pred)\n",
    "        \n",
    "        # Save metrics\n",
    "        row = {\n",
    "            'Model Used': model_name,\n",
    "            'Training MSE': train_mse,\n",
    "            'Training RMSE': train_rmse,\n",
    "            'Training MAE': train_mae,\n",
    "            'Training R¬≤': train_r2,\n",
    "            'Testing MSE': test_mse,\n",
    "            'Testing RMSE': test_rmse,\n",
    "            'Testing MAE': test_mae,\n",
    "            'Testing R¬≤': test_r2,\n",
    "            'Best Params': grid_search.best_params_\n",
    "        }\n",
    "        metrics_list.append(row)\n",
    "\n",
    "    # Convert the metrics into a Dataframe\n",
    "    metrics_df = pd.DataFrame(metrics_list)\n",
    "    return metrics_df, best_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_df, best_models = train_and_evaluate_models(X_train, y_train, X_test, y_test, scaler)\n",
    "metrics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function to displat the evaluation results\n",
    "def display_evaluation_results(metrics_df):\n",
    "    metrics_df.set_index('Model Used')[['Training R¬≤', 'Testing R¬≤']].plot(kind='bar', figsize=(12,8))\n",
    "\n",
    "    plt.title('Model Comparison')\n",
    "    plt.xlabel('Model')\n",
    "    plt.ylabel('Score')\n",
    "    plt.xticks(rotation=0)\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "display_evaluation_results(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings\n",
    "\n",
    "#### 1. ExtraTree\n",
    "- **Training Performance**: High MSE and RMSE indicate overfitting. The model performs very well on the training set (R¬≤ = 0.98) but less so on the test set (R¬≤ = 0.73).\n",
    "- **Testing Performance**: Although R¬≤ is relatively high, the high MSE and RMSE on the test set suggest variability in predictions.\n",
    "\n",
    "#### 2. XGBoost\n",
    "- **Training Performance**: Shows excellent fit on the training data (R¬≤ = 0.95).\n",
    "- **Testing Performance**: Good performance on the test data (R¬≤ = 0.75). XGBoost balances performance between training and testing data, making it a reliable model.\n",
    "\n",
    "#### 3. ElasticNet\n",
    "- **Training Performance**: Moderate performance on the training set (R¬≤ = 0.47).\n",
    "- **Testing Performance**: Poor performance on the test set (R¬≤ = 0.60), indicating it might not capture the underlying patterns effectively.\n",
    "\n",
    "#### 4. SVR\n",
    "- **Training Performance**: Decent fit on the training data (R¬≤ = 0.31).\n",
    "- **Testing Performance**: Poor performance on the test set (R¬≤ = 0.29). High testing errors suggest overfitting or model inadequacy.\n",
    "\n",
    "#### 5. GradientBoosting\n",
    "- **Training Performance**: Good fit on the training data (R¬≤ = 0.89).\n",
    "- **Testing Performance**: Strong performance on the test set (R¬≤ = 0.75). This model shows a good balance, similar to XGBoost, with lower test errors.\n",
    "\n",
    "#### 6. Lasso\n",
    "- **Training Performance**: Moderate fit on the training data (R¬≤ = 0.49).\n",
    "- **Testing Performance**: Below-average performance on the test set (R¬≤ = 0.45), indicating it might struggle with capturing complex patterns.\n",
    "\n",
    "#### 7. Ridge\n",
    "- **Training Performance**: Similar to Lasso with moderate fit (R¬≤ = 0.49).\n",
    "- **Testing Performance**: Same as Lasso (R¬≤ = 0.45). This indicates that regularization techniques like Ridge and Lasso might need further tuning.\n",
    "\n",
    "#### 8. AdaBoost\n",
    "- **Training Performance**: Lower fit on the training data (R¬≤ = 0.59).\n",
    "- **Testing Performance**: Moderate performance on the test set (R¬≤ = 0.57). While not the best, it shows reasonable generalization capabilities.\n",
    "\n",
    "### Best Performing Models\n",
    "- **XGBoost and GradientBoosting** stand out as the best models based on the Testing R¬≤ values (both around 0.75) and relatively lower testing errors. These models demonstrate a good balance between bias and variance, making them suitable for housing price prediction.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Curves "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the learning curves\n",
    "def plot_learning_curve(estimator, X, y, cv=5, n_jobs=None, train_sizes = np.linspace(0.1, 1.0, 5), scoring='r2'):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=scoring)\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1) \n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(train_sizes, train_scores_mean, label='Training Scores')\n",
    "    plt.plot(train_sizes, test_scores_mean, label='Testing Scores')\n",
    "    plt.xlabel('Training examples')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(False)\n",
    "    plt.show\n",
    "\n",
    "    \n",
    "\n",
    "#call the learning curve function for each model\n",
    "for model_name, best_model in best_models.items():\n",
    "    print(f'Learning CUrve for {model_name}')\n",
    "    plot_learning_curve(best_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "# Train a regression model (e.g., RandomForestRegressor)\n",
    "model = gbr\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Create a SHAP explainer\n",
    "explainer = shap.Explainer(model, X_train)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer(X_test)\n",
    "\n",
    "# Plot SHAP summary\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "shap.plots.force(shap_values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-gg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
